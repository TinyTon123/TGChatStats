# -*- coding: utf-8 -*-

from telethon.sync import TelegramClient
from telethon.tl.types import PeerChannel
from datetime import datetime, timedelta, date
import datetime
import pandas as pd

import re
import emoji

from pymorphy2 import MorphAnalyzer
import nltk
from nltk import word_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from sqlalchemy import create_engine

pd.options.display.max_colwidth = 150
pd.set_option('chained_assignment', None)

# id –∏ hash —Å —Å–∞–π—Ç–∞ https://my.telegram.org/apps
api_id =
api_hash = ''
phone = ''
client = TelegramClient(phone, api_id, api_hash)

# –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (–ø–∞—Ä–æ–ª—å, –µ—Å–ª–∏ 2FA)
client.start(password='')

# –∑–∞–ø—Ä–æ—Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–∞–Ω–∞–ª–µ (–≥—Ä—É–ø–ø–µ)
chat_title = client.get_entity(PeerChannel())

def get_statistics():
    # –∑–∞–ø—Ä–æ—Å —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–≤–∞ –¥–Ω—è
    messages = client.get_messages(
        entity=PeerChannel(channel_id=),  # limit=?
        offset_date=datetime.datetime.now()
                    .replace(hour=0, minute=0, second=0, microsecond=0) -
                    datetime.timedelta(hours=51), reverse=True
    )

    # –∑–∞–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ (–¥—Ñ)
    df = pd.DataFrame(
        columns=['message_id', 'date', 'from_id', 'from_name', 'from_username', 'message', 'reactions_count'])

    # —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥—Ñ
    for i in range(len(messages)):
        if not messages[i].fwd_from and type(messages[i]) != 'telethon.tl.patched.MessageService':

            # —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —é–∑–µ—Ä–Ω–µ–π–º–∞ –∏ —Ö—ç–Ω–¥–ª–µ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            if messages[i].from_id:
                # –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                from_id = client.get_entity(messages[i].from_id)
                handler = '@' + str(from_id.username)
                name = from_id.first_name + ' ' + from_id.last_name if from_id.last_name else from_id.first_name
                user_id = messages[i].from_id.user_id
            else:
                # –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π –∞–¥–º–∏–Ω–æ–≤ –æ—Ç –∏–º–µ–Ω–∏ –≥—Ä—É–ø–ø—ã
                handler, user_id = '--', '--'
                name = client.get_entity(messages[i].peer_id).title

            # –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è 3 —á–∞—Å–∞, —á—Ç–æ–±—ã –æ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–æ –º–æ—Å–∫–æ–≤—Å–∫–æ–º—É –≤—Ä–µ–º–µ–Ω–∏
            actual_date = messages[i].date + datetime.timedelta(hours=3)

            # —Å–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ–æ–±—â–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å—Å—è –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–π–∫–æ–π –≤ –¥—Ñ
            msg_info = [messages[i].id, actual_date, user_id, name, handler, messages[i].message]

            # –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ä–µ–∞–∫—Ü–∏–π
            if messages[i].reactions:
                reactions_count = 0

                # –ø–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∞–∫—Ü–∏–π
                for cnt in messages[i].reactions.results:
                    reactions_count += cnt.count

                # –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫
                msg_info.append(reactions_count)
            else:
                msg_info.append(0)

            # –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –¥—Ñ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ–æ–±—â–µ–Ω–∏–∏
            df.loc[i] = msg_info

    # –ü—Ä–∏–≤–æ–¥–∏–º —Å—Ç–æ–ª–±–µ—Ü —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –∫ —Ñ–æ—Ä–º–∞—Ç—É date.
    df['date'] = df['date'].apply(lambda x: datetime.date(x.year, x.month, x.day))

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ –¥–∞—Ç–µ: –∑–∞ –≤—á–µ—Ä–∞ –∏ –∑–∞ –ø–æ–∑–∞–≤—á–µ—Ä–∞.
    yest = date.today() - timedelta(days=1)
    two_days_before = date.today() - timedelta(days=2)
    yest_fmt = yest.strftime(format='%d-%m-%Y')

    messages_yest = df[df['date'] == yest]
    messages_2_days_before = df[df['date'] == two_days_before]

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π

    # –î–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π —Å –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å—é

    def top_thankful(row):
        regex = r'–ø–∞—Å–∏–±|—Å–ø—Å|thank|–±–ª–∞–≥–æ–¥–∞—Ä(?=—é|–∏–º|–µ–Ω|–Ω–∞)'
        if re.findall(regex, str(row), flags=re.IGNORECASE):
            result = 1
        else:
            result = 0
        return result

    # –î–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏

    def top_inquisitive(row):
        regex = r'(?<=[ \w])\?\B'
        if re.findall(regex, str(row)):
            result = 1
        else:
            result = 0
        return result

    # –î–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π —Å –º–∞—Ç–æ–º

    def top_swearers(row):
        regex = r"\b(?<!-)(?:(?:[–∞–æ]|—Å–Ω–∏|[–¥–Ω][–∏–æ–∞][ -]?|–ø[–∞–æ])?[—Ö–∑–∫–±]—É-?[ –π#*@]?(?:—Ü–∞|[–∏–µ—é][–≤—Ü–Ω]?|–µ—Ç[—É–∞—å]|–Ω[—è—é–µ]|–µ?–ª[–∏—å–µ](?:–∞—Ä–¥–∞?)?\b|—è(?: [—Ç—Å]–µ–±–µ)?(?!–Ω)| [—Ç–≤–Ω]–∞–º| [—Ç—Å]–µ–±–µ))(?!—Ç—å)|(?:–ø[–∏–µ—ë]*[—Å–∑\*][–¥–ª–∂])|(?<![–ª—Ä—Ç–Ω—à—á–¥])(?:[–µ—ë][–±\*]–ª?[–∞–æ]?[–Ω–ª](?:–∞—Ç|—å–∫–æ|))|(?<![–≤–¥—Ä–Ω—á–ª—Ç—à—Ü])(?:[–µ—ë][–±\*](?:[—É–µ–∞—Å]*—Ç—å?|–Ω?—É(?:—á|—á–∫.)?|[–∞–æ]—à|—Ü—á|–∏|(?: —Ç–≤| –≤–∞|—ã)|–µ[–π–Ω]))|\b(?:–ø–æ|–Ω–∞|—Å)?[—Ö–∑]—É[—è–π]|(?:—Å—É+–∫(?:[–∞–∏]|–æ–π|–∞—ç–ª(?:—å|–µ–º?)(?:—á–∏–∫)?|–∞–º–∏?)?)\b|(?<![—Ä–∞—É–µ–æ–º])(?:—É–∫–∞)?(?:–±–ª[—è*—ç–µ][—Ç–¥]?(?:—å—é?|—è*|–µ–π|—è–º–∏?|—Å–∫)?)(?![–¥–º–Ω–∫—Å–≤—Ñ–π])|(?<!—É)(?:–º—É–¥–∞?[^—Ä ])|–Ω[–µ–∏] –∏–ø(?:[—É–µ]—Ç?)|(?<!—Å–∫–∏)–ø–∏–¥[–∞–æ–µ]—Ä|(?:–¥–æ–ª–±[–∞–æ]|[—É—å—ä]|[–Ω–∑]–∞)[–µ—ë][–±–ø]|(?<!—Ä)—Å—É—á(?:–∞—Ä|—å|–∏–π)|–∑–±—Å"
        findall = re.findall(regex, str(row), flags=re.IGNORECASE)
        result = len(findall) if findall else 0
        return result

    # –î–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏–π (–æ—á–∏—â–∞–µ–º –æ—Ç —ç–º–æ–¥–∂–∏ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞)

    def words(row):
        de_emoj = emoji.demojize(str(row))
        regex = r":[a-z_]*:|{?'type'(?:\: ?)*|{?'code'(?:\: ?)*|{?'text'(?:\: ?)*|'|{|}"
        clean_text = re.sub(regex, '', de_emoj)
        strip = clean_text.strip()
        regex = r'\b[–∞-—è—ë-]+\b'
        words_count = re.findall(regex, strip, flags=re.IGNORECASE)
        return len(words_count)

    messages_yest['thanks'] = messages_yest['message'].apply(top_thankful)
    messages_yest['questions'] = messages_yest['message'].apply(top_inquisitive)
    messages_yest['swearer'] = messages_yest['message'].apply(top_swearers)
    messages_yest['symbols'] = messages_yest['message'].apply(lambda x: len(str(x)))
    messages_yest['words'] = messages_yest['message'].apply(words)

    # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö

    # –¢–æ–ø —Å–∞–º—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä—á–∏–≤—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤

    msgs_grp = (
        messages_yest[messages_yest['from_id'] != ]
        .groupby(['from_name', 'from_id', 'from_username'])
        ['message'].count().sort_values(ascending=False).reset_index().head(10)
    )

    # –¢–æ–ø —Å–∞–º—ã—Ö –±–ª–∞–≥–æ–¥–∞—Ä–Ω—ã—Ö

    thanks_grp = (
        messages_yest[(messages_yest['thanks'] == 1) & (messages_yest['from_name'] != 'TinyüçÄTon') &
                      (messages_yest['from_id'] != )]
        .groupby(['from_name', 'from_id', 'from_username'])['thanks']
        .count().sort_values(ascending=False).reset_index().head(5)
    )

    # –¢–æ–ø —Å–∞–º—ã—Ö –ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω—ã—Ö

    questions_grp = (
        messages_yest[(messages_yest['questions'] == 1) & (messages_yest['from_id'] != )]
        .groupby(['from_name', 'from_id', 'from_username'])['questions']
        .count().sort_values(ascending=False).reset_index().head(3)
    )

    # –¢–æ–ø —Å–∞–º—ã—Ö –º–æ–ª—á–∞–ª–∏–≤—ã—Ö –∏–∑ —Ç–µ—Ö, –∫—Ç–æ –æ—Ç–ø—Ä–∞–≤–∏–ª —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –∑–∞ –¥–µ–Ω—å

    most_reticent = (
        messages_yest[messages_yest['from_id'] != ]
        .groupby(['from_name', 'from_id', 'from_username'])
        ['message'].count().sort_values().reset_index().head(5)
    )

    # –¢–æ–ø –º–∞—Ç–µ—Ä—â–∏–Ω–Ω–∏–∫–æ–≤

    swearers_grp = (
        messages_yest[(messages_yest['swearer'] > 0) &
                      (messages_yest['from_id'] != ) &
                      (messages_yest['from_name'] != 'TinyüçÄTon')]
        .groupby(['from_name', 'from_id', 'from_username'])['swearer']
        .sum().sort_values(ascending=False).reset_index().head(5)
    )

    # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è

    longest_post = (
        messages_yest[(messages_yest['from_name'] != 'TinyüçÄTon') &
                      (messages_yest['from_id'] != )]
        [['from_name', 'from_id', 'from_username', 'words', 'message_id']]
        .sort_values(by='words', ascending=False).head(3)
    )

    # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–∞–º–æ–≥–æ "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ" —Å–æ–æ–±—â–µ–Ω–∏—è

    most_engaging = (
        messages_yest[(messages_yest['from_id'] != ) &
                      ~(messages_yest['message'].apply(lambda x: True if any(st in str(x)
                                                                             for st in ['"–∫–æ–º–Ω–∞—Ç–∞ –∏—Å—Ç–æ—á–∞—é—â–∞—è —Å–≤–µ—Ç"',
                                                                                        '–ü–æ–≥–æ–¥–∞ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è, ']) else False))]
        [['from_name', 'from_id', 'from_username', 'reactions_count', 'message_id']]
        .sort_values(by='reactions_count', ascending=False).head(3)
    )

    # –†–∞—Å—á–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ–π –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑–Ω–∏—Ü—ã –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –æ–±—â–∞–≤—à–∏—Ö—Å—è –∑–∞ –¥–≤–∞ –¥–Ω—è

    unique_yest = messages_yest['from_id'].unique()
    unique_2_days_before = messages_2_days_before['from_id'].unique()
    diff_chatting = len(unique_yest) - len(unique_2_days_before)
    diff = round((len(messages_yest) - len(messages_2_days_before)) / len(messages_2_days_before) * 100, 1)

    # –†–∞—Å—á–µ—Ç –º–µ–¥–∏–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤

    mdn_words = int(messages_yest['words'].median())
    mdn_symbols = int(messages_yest['symbols'].median())

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è

    if diff_chatting == 0:
        chatting = '—Å—Ç–æ–ª—å–∫–æ –∂–µ, —Å–∫–æ–ª—å–∫–æ –∏ –¥–Ω–µ–º —Ä–∞–Ω–µ–µ'
    elif diff_chatting > 0:
        chatting = f'–Ω–∞ {diff_chatting} –±–æ–ª—å—à–µ, —á–µ–º –¥–Ω–µ–º —Ä–∞–Ω–µ–µ'
    else:
        chatting = f'–Ω–∞ {abs(diff_chatting)} –º–µ–Ω—å—à–µ, —á–µ–º –¥–Ω–µ–º —Ä–∞–Ω–µ–µ'

    people = '—á–µ–ª–æ–≤–µ–∫–∞' if len(unique_yest) % 10 in (2, 3, 4) else '—á–µ–ª–æ–≤–µ–∫'

    wrd = longest_post.iloc[0][3]
    rct = most_engaging.iloc[0][3]

    def wordform(cnt):
        if cnt % 100 in (11, 12, 13, 14):
            wrdform, reactions = '—Å–ª–æ–≤', '—Ä–µ–∞–∫—Ü–∏–π'
        else:
            if cnt % 10 == 1:
                wrdform, reactions = '—Å–ª–æ–≤–æ', '—Ä–µ–∞–∫—Ü–∏—è'
            elif cnt % 10 in (2, 3, 4):
                wrdform, reactions = '—Å–ª–æ–≤–∞', '—Ä–µ–∞–∫—Ü–∏–∏'
            else:
                wrdform, reactions = '—Å–ª–æ–≤', '—Ä–µ–∞–∫—Ü–∏–π'

        result = wrdform if cnt == wrd else reactions

        return result

    msgs = '–º–µ–Ω—å—à–µ' if diff < 0 else '–±–æ–ª—å—à–µ'

    # –†–∞—Å—á–µ—Ç —Ç–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –æ—Ç —ç–º–æ–¥–∂–∏ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è

    def full_clean_text(row):
        de_emoj = emoji.demojize(str(row))
        regex = r"""[‚Äî'{}\\:;,.()\[\]a-z@\/?\+!0-9&_=#$%^*~\"‚Ä¶¬´¬ª-]+|\n"""
        clean_text = re.sub(regex, ' ', de_emoj, flags=re.IGNORECASE)
        return clean_text

    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤

    messages_yest_wo_me = messages_yest[messages_yest['from_name'] != 'TinyüçÄTon']
    messages_yest_wo_me['clean_text'] = messages_yest_wo_me['message'].apply(full_clean_text)
    morph = MorphAnalyzer()
    clean_text = []
    for message in messages_yest_wo_me['clean_text']:
        token = list(map(lambda x: morph.normal_forms(x.strip())[0], message.lower().split()))
        clean_text.extend(token)

    clean_text = ' '.join(clean_text)
    clean_text = clean_text.replace('—ë', '–µ')

    text_tokens = word_tokenize(clean_text)
    text_tokens = nltk.Text(text_tokens)

    # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –∏–∑ —Ç–æ–ø–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤, –Ω–µ –Ω–µ—Å—É—â–∏—Ö —Å–º—ã—Å–ª –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–º–∞—Ç–∏–∫–∏ –æ–±—â–µ–Ω–∏—è –≤ —á–∞—Ç–µ

    russian_stopwords = stopwords.words("russian")
    russian_stopwords.extend(['—ç—Ç–æ', '–µ—â–µ', '–æ—á–µ–Ω—å', '–≤–æ–æ–±—â–µ', '–ø—Ä–æ—Å—Ç–æ', '–∫—Å—Ç–∞—Ç–∏', '–≤—Ä–æ–¥–µ',
                              '—Ö–∑', '—Ç–µ–±–µ', '–æ–∫', '–ø–æ–∫–∞', '—Ç–∏–ø–∞', '–∫–∞–∫–∏–µ', '–∫–∞–∂–µ—Ç—Å—è',
                              '—Å–µ–≥–æ–¥–Ω—è', '–∑–∞–≤—Ç—Ä–∞', '–ø–æ—ç—Ç–æ–º—É', '—Ç–∞–∫–æ–π', '–≤—á–µ—Ä–∞', '–Ω–æ—Ä–º',
                              '—Å–ø–∞—Å–∏–±–æ', '–±–ª–∏–Ω', '–∞–≥–∞', '—â–∞', '–ø—Ä—è–º', '–∫–æ—Ç–æ—Ä—ã–π', '–≤–µ—Å—å',
                              '—Å–≤–æ–π', '—Ç–∏–ø', '–º–æ–π', '—Ç–≤–æ–π', '–Ω–∞—à', '–≤–∞—à', '—Å–∞–º—ã–π', '–æ–π',
                              '–º–æ—á—å', '–ø–æ—á–µ–º—É', '—Ä'])

    words_frequency_wo_sw = [word for word in text_tokens if word not in russian_stopwords]
    words_frequency_wo_sw = FreqDist(words_frequency_wo_sw)
    top_10_words = words_frequency_wo_sw.most_common(10)

    top_10_words_str = ''
    for i in range(len(top_10_words)):
        top_10_words_str += top_10_words[i][0] + ', '
    # top_10_words_str[:-2]

    # –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ —Å –∫–æ—Ä–Ω–µ–º –ø–∏–¥(–æ/–∞)—Ä
    word_list = words_frequency_wo_sw.most_common()
    pidor = 0
    for k, v in word_list:
        if k.count('–ø–∏–¥–æ—Ä') or k.count('–ø–∏–¥–∞—Ä') or k.count('–ø–∏–¥—Ä'):
            pidor += v

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –ø–æ–ª—É—á–∏–≤—à–µ–µ—Å—è —á–∏—Å–ª–æ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –Ω–µ–¥–µ–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞

    engine = create_engine('sqlite:///C:/SQLite/dbs/tg_chat.db')
    engine.connect()

    query = f'''DELETE FROM pidors
                       WHERE dt = '{yest}'
             '''
    engine.execute(query)

    pidors_cnt = pd.DataFrame([[yest, pidor]],
                              columns=['dt', 'cnt'])

    pidors_cnt.to_sql(name='pidors', con=engine, if_exists='append', index=False)

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –æ—Ç—á–µ—Ç–æ–º

    text = f"""‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è\n\n**"{chat_title.title}"**\n\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞—Ç–∞ –∑–∞ –≤—á–µ—Ä–∞ ({yest_fmt}).\n\n"""
    text += f"""–û—Å—Ç–∞–≤–ª–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages_yest)},\n—á—Ç–æ **–Ω–∞ {str(abs(diff)).replace('.', ',')}% {msgs}**, —á–µ–º –¥–Ω–µ–º —Ä–∞–Ω–µ–µ.\n\n"""
    text += f"""–í—Å–µ–≥–æ –æ–±—â–∞–ª–∏—Å—å: **{len(unique_yest)} {people}**\n({chatting}).\n\n**–ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞** —Å–æ–æ–±—â–µ–Ω–∏—è:\n"""
    text += f"""—Å–ª–æ–≤ ‚Äî {mdn_words}, —Å–∏–º–≤–æ–ª–æ–≤ ‚Äî {mdn_symbols}\n\n–¢–æ–ø-10 **—Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö** —Å–ª–æ–≤:\n{top_10_words_str[:-2]}\n\n"""
    text += f"""__–°–∞–º—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—á–∏–≤—ã–µ –≤ —á–∞—Ç–µ__:\n"""
    text += f"""1) **{msgs_grp.iloc[0][0]}** ({msgs_grp.iloc[0][2]}) (—Å–æ–æ–±—â–µ–Ω–∏–π: {msgs_grp.iloc[0][3]}) ü•á\n"""
    text += f"""2) **{msgs_grp.iloc[1][0]}** ({msgs_grp.iloc[1][2]}) (—Å–æ–æ–±—â–µ–Ω–∏–π: {msgs_grp.iloc[1][3]}) ü•à\n"""
    text += f"""3) **{msgs_grp.iloc[2][0]}** ({msgs_grp.iloc[2][2]}) (—Å–æ–æ–±—â–µ–Ω–∏–π: {msgs_grp.iloc[2][3]}) ü•â\n"""
    text += f"""4) **{msgs_grp.iloc[3][0]}** ({msgs_grp.iloc[3][2]}) (—Å–æ–æ–±—â–µ–Ω–∏–π: {msgs_grp.iloc[3][3]})\n"""
    text += f"""5) **{msgs_grp.iloc[4][0]}** ({msgs_grp.iloc[4][2]}) (—Å–æ–æ–±—â–µ–Ω–∏–π: {msgs_grp.iloc[4][3]})\n\n"""
    text += f"""__–°–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ__:\n"""
    text += f"""**{longest_post.iloc[0][0]}** ({longest_post.iloc[0][2]}) ‚Äî {longest_post.iloc[0][3]} {wordform(wrd)} (t.me/c/1403290431/{longest_post.iloc[0][4]})\n\n"""
    text += f"""__–°–∞–º–æ–µ "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ" —Å–æ–æ–±—â–µ–Ω–∏–µ__:\n"""
    text += f"""**{most_engaging.iloc[0][0]}** ({most_engaging.iloc[0][2]}) ‚Äî {most_engaging.iloc[0][3]} {wordform(rct)} (t.me/c/1403290431/{most_engaging.iloc[0][4]})\n\n"""
    text += f"""__–°–∞–º—ã–µ –ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω—ã–µ__:\n"""

    for i in range(3):
        text += f'{i + 1}) **{questions_grp.iloc[i][0]}** ({questions_grp.iloc[i][2]}) (–≤–æ–ø—Ä–æ—Å–æ–≤: {questions_grp.iloc[i][3]})\n'

    text += '\n__–°–∞–º—ã–µ –±–ª–∞–≥–æ–¥–∞—Ä–Ω—ã–µ__:\n'

    try:
        for i in range(3):
            text += f'{i + 1}) **{thanks_grp.iloc[i][0]}** ({thanks_grp.iloc[i][2]}) (–ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏–ª(–∞) —Ä–∞–∑: {thanks_grp.iloc[i][3]})\n'
    except IndexError:
        pass

    text += '\n__–¢–æ–ø –º–∞—Ç–µ—Ä—â–∏–Ω–Ω–∏–∫–æ–≤__:\n'

    if len(swearers_grp):
        try:
            for i in range(3):
                text += f'{i + 1}) **{swearers_grp.iloc[i][0]}** ({swearers_grp.iloc[i][2]}) (–≤—ã—Ä—É–≥–∞–ª—Å—è(-–∞—Å—å) —Ä–∞–∑: {swearers_grp.iloc[i][3]})\n'
        except IndexError:
            pass
    else:
        text += '–í—á–µ—Ä–∞ –Ω–∏–∫—Ç–æ –Ω–µ —Ä—É–≥–∞–ª—Å—è! –î–æ–∂–∏–ª–∏!\n'

    text += '\n__–°–∞–º—ã–µ –Ω–µ—Ä–∞–∑–≥–æ–≤–æ—Ä—á–∏–≤—ã–µ –∏–∑ –æ–±—â–∞–≤—à–∏—Ö—Å—è__:\n'

    for i in range(5):
        text += f'{i + 1}) **{most_reticent.iloc[i][0]}** ({most_reticent.iloc[i][2]}) (—Å–æ–æ–±—â–µ–Ω–∏–π: {most_reticent.iloc[i][3]})\n'

    text += f"\n__–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∏–¥–æ—Ä—Å–∫–∏–π —Å—á–µ—Ç—á–∏–∫ (—á—Ç–æ–±—ã –∫–æ–µ-–∫—Ç–æ —Ö–æ—Ä–æ—à–æ —Å–ø–∞–ª–∞)__:\n**–ü–∏–¥–æ—Ä–æ–≤ –∑–∞ –¥–µ–Ω—å** ‚Äî {pidor}"""

    # –ø–ª–∞–Ω–∏—Ä—É–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏—é –æ—Ç—á–µ—Ç–∞ –Ω–∞ 12:00, –µ—Å–ª–∏ —Å–∞–º –æ—Ç—á–µ—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –¥–æ —ç—Ç–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –∏ –º–æ–º–µ–Ω—Ç–∞–ª—å–Ω–æ, –µ—Å–ª–∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –ø–æ—Å–ª–µ 12

    if datetime.datetime.now() < datetime.datetime.now().replace(hour=12, minute=0, second=0, microsecond=0):
        client.send_message(entity=PeerChannel(channel_id=), message=text,
                            schedule=datetime.datetime.now().replace(hour=12, minute=0, second=0, microsecond=0) -
                                     datetime.datetime.now())
    else:
        client.send_message(entity=PeerChannel(channel_id=), message=text)


if __name__ == '__main__':
    # main()
    get_statistics()
